# Nets 2130 Homework 1 Writeup
## By Via Liu
## Overview
Three platforms are tried out with different aspects of crowdsourcing: Zooniverse, Google Crowdsourcing, and Mercor.

This writeup explores my experiences with three distinct crowdsourcing platforms, each offering unique insights into the field. From citizen science projects on Zooniverse to AI-driven hiring processes on Mercor, I've engaged with a diverse range of tasks and methodologies. The following sections detail my observations, challenges encountered, and reflections on the potential impacts of these platforms on scientific research, data collection, and recruitment processes.

# Zooniverse

## Project Selection and Tasks

I selected six diverse projects on Zooniverse to explore different aspects of crowdsourcing relating to sciences. Here is a screenshot of the progress of my projects:

<img width="1257" alt="Screenshot 2024-09-29 at 10 07 52 AM" src="https://github.com/user-attachments/assets/4c97f28c-065e-4d3e-a8a7-396e8b7b798f">


1. **Exosteroids and Galaxy Zoo**: From my initial research, astronomy-focused projects are among Zooniverse's most popular and well-known (and ones that lead to the most profound results). These are the projects that can be most representative to begin with. The two specific projects involve volunteers identifying stars and asteroids, which very much suits my interest. 

2. **Shark Spy and Wildlife Watch**: As a wildlife enthusiast, I chose these projects to apply my animal knowledge (such as birdwatching) to classification tasks. They offered insight into how crowdsourcing works in biodiversity research.

3. **Sudan Road Access**: From class, I was very impressed with how crowdsourcing can help with projects in underdeveloped countries. This project stood out for its real-world impact, helping to map roads and infrastructure for aid delivery in Sudan. I wish to understand how crowdsourcing can directly benefit humanitarian efforts through this project.

4. **Reading Emotions**: This psychology-related project caught my attention as it aligns with my minor in psychology. It involves more complex, and human centric tasks compared to simple classification. I would also like to know whether humans can be effective in sentimental analysis even after the emergence of generative AI> 

The tasks primarily involved classification, which humans can often perform more accurately than machines in certain contexts. For example:

- Identifying sharks or birds in images (Shark Spy and Wildlife Watch)
- Classifying galaxy types and asteroid characteristics (Galaxy Zoo and Exosteroids)
- Inferring emotions from written statements (Reading Emotions)
- Identifying infrastructure features like bridges and unpaved roads in satellite imagery (Sudan Road Access)

These tasks highlight a key strength of human crowdsourcing: the ability to make nuanced judgments that may be challenging for AI. For instance, distinguishing between a shark and other fish in murky water, or interpreting the emotional subtext of a written statement.

## Task Instructions and User Experience

Most project instructions were clear and easy to follow, with step-by-step guides available for each task. The UI is also very friendly: whenever a volunteer opens a task, he or she would be directed to the instructions which in clude forward and backward buttons to show each step consecutively. Moreover, the instructions also include multiple images for better comprehension. However, I encountered some usability issues:

1. In Shark Spy and Wildlife Watch, while the basic instructions were clear, identifying specific species required clicking through multiple images for reference. It's very difficult for me to identify the sharks as there is only a single image provided for each species and the images are very small. This process could be streamlined.

<img width="1306" alt="Screenshot 2024-09-29 at 9 40 07 AM" src="https://github.com/user-attachments/assets/f085a607-c90b-4986-b75f-307f84cd86e0">


2. The Wildlife Watch interface presented an unnecessarily large set of animal options, most of which were irrelevant to the actual tasks. This cluttered the UI and made bird identification more cumbersome than necessary. Moreover, I have to click in the species name to find the image of a species, which is very inconvenient. 

<img width="1305" alt="Screenshot 2024-09-29 at 9 21 17 AM" src="https://github.com/user-attachments/assets/88f74d10-978b-40c0-bfb8-83fa9a69260d">

<img width="423" alt="Screenshot 2024-09-29 at 9 30 48 AM" src="https://github.com/user-attachments/assets/d885eebc-d109-4981-bf43-fbb85c30d126">


3. The Sudan Road Access project initially posed challenges in distinguishing between roads and rivers in satellite imagery. This highlights the importance of providing clear visual examples and guidelines for more complex tasks.

<img width="1100" alt="Screenshot 2024-09-29 at 11 18 49 AM" src="https://github.com/user-attachments/assets/f14fbce0-9145-4661-8881-3059b8886dfd">


Despite these minor issues, Zooniverse generally provides a user-friendly experience with easily accessible instructions and intuitive interfaces for most projects.

## Completed Projects and Publications

Zooniverse's model of citizen science has led to numerous scientific publications. One notable example is the discovery of "Green Pea" galaxies, detailed in the paper "Galaxy Zoo Green Peas: discovery of a class of compact extremely star-forming galaxies."

<img width="638" alt="Screenshot 2024-09-29 at 11 20 02 AM" src="https://github.com/user-attachments/assets/0a09c87c-e668-43af-9afe-1f814fca7444">


This project showcases the unique advantage of human classifiers in scientific research. Volunteers were able to identify and investigate unusual objects that might have been overlooked by automated systems. The collective effort of citizen scientists helped establish the common characteristics of these rare galaxies, leading to a significant astronomical discovery.

This example illustrates how crowdsourcing can:

1. Leverage human pattern recognition skills for scientific breakthroughs
2. Enable large-scale data analysis that would be impractical for a small research team
3. Engage the public in the scientific process, potentially increasing science literacy and interest

## Time Investment

I spent approximately 30-40 minutes on most projects. For less engaging ones, I spend about 1t minutes. Individual tasks typically took around 15 seconds to complete.

This time investment reveals both strengths and limitations of the Zooniverse model:

- **Strength**: Quick, bite-sized tasks allow for easy participation and rapid data collection.
- **Limitation**: The repetitive nature of tasks can lead to fatigue or boredom, potentially affecting long-term volunteer retention.

## Volunteer Motivation

Through my experience, I realize that Zooniverse relies entirely on volunteers. This is very different from Amazon Mechanical Turk and can result in Zooniverse having a completely different audience than Turk. From my opinion, some of the points of motivation could be:

1. **Scientific Impact**: Many projects directly contribute to research, giving volunteers a sense of purpose and connection to scientific discovery. People choosing Zooniverse alreadya are those who don't care for making an income out of crowdsourcing (which differs a lot from Turk) and these impactful projects can serve as a strong motivation for them to continue. 

2. **Real-World Applications**: Projects like Sudan Road Access offer tangible humanitarian benefits. These benefits can often times not be measurable with money and is more suitable for a non-profit platform like Zooniverse.

3. **User Experience**: Zooniverse's very well-designed interface and clear instructions lower the barrier to entry and make participation enjoyable. This contrasts sharply to Turk's old-fashion design. Moreover, the tasks are very easily navigable and one can easily look for tasks of similar topics. 

4. **Topic Interest**: The diverse range of projects allows volunteers to contribute to fields they're passionate about, from astronomy to wildlife conservation.

5. **Eady Signups**: Signing up is very easy and straightforward.


However, the platform faces challenges in maintaining long-term engagement. The repetitive nature of tasks can lead to boredom, as I experienced after about 30 minutes of continuous work. To address this, Zooniverse could consider:

- Implementing more varied task types within projects
- Offering more frequent feedback on the impact of contributions
- Developing a robust community system to foster social connections among volunteers

## Ethical Considerations and Future Directions

While Zooniverse's volunteer model avoids many of the ethical concerns associated with paid crowdsourcing platforms, it raises other considerations:

1. **Data Privacy**: How is volunteer-submitted data protected and used?
2. **Intellectual Property**: How are volunteers credited for their contributions to scientific discoveries?
3. **Inclusivity**: Are there barriers to participation that might exclude certain groups?

Looking ahead, Zooniverse and similar platforms could explore:

1. Enhanced feedback mechanisms to show volunteers the direct impact of their work
2. Integration of machine learning to complement human efforts and tackle more complex tasks
3. Expanded educational resources to deepen volunteers' understanding of the scientific process
4. Partnerships with schools and universities to integrate citizen science into curricula

In conclusion, Zooniverse demonstrates the potential of citizen science crowdsourcing to advance scientific research and address real-world challenges. By leveraging human cognitive abilities and intrinsic motivation, it offers a unique model that complements other forms of crowdsourcing and traditional research methodologies.

# Google Crowdsource

## Project Selection and Task Description

For this assignment, I focused solely on the Image Recognition task within Google Crowdsource. This was not by choice, but due to the platform's limited availability of tasks during my access period. The primary objective was to identify specific objects within images, a task that aligns with Google's efforts to improve its image recognition algorithms.

<img width="794" alt="Screenshot 2024-09-29 at 12 22 30 PM" src="https://github.com/user-attachments/assets/62ec302f-aa8e-47a6-a606-dc5511600435">


This limitation in task variety reveals an interesting aspect of Google Crowdsource: the platform seems to prioritize specific data collection needs at different times, which could potentially lead to a less diverse user experience.

## User Interface and Task Completion

Google Crowdsource presents a streamlined and user-friendly interface. The task instructions were clear and easy to follow, allowing for efficient completion of image recognition tasks. Over the course of my participation, I contributed to more than 1,000 image recognitions.

The UI/UX design of Google's annotation tool is straightforward and intuitive, enabling users to select images directly without unnecessary steps. This efficiency is crucial for maintaining user engagement in repetitive tasks. However, there's room for improvement in the overall platform navigation. The tasks could be more prominently displayed, separate from the achievements section, to enhance accessibility.

## User Motivation

Google Crowdsource employs a badge system to motivate users, which I found both engaging and strategic. During my participation, I earned several badges:

1. Spotter: Silver badge (over 1,000 recognitions)
2. Good Samaritan: 200 contributions
3. Contributor: Level 10 (1,000 contributions)

<img width="480" alt="Screenshot 2024-09-29 at 12 22 44 PM" src="https://github.com/user-attachments/assets/e19cce8c-c6df-4d07-ba03-eabea2134563">

<img width="439" alt="Screenshot 2024-09-29 at 12 22 39 PM" src="https://github.com/user-attachments/assets/c59cda61-ac7e-4d9a-a219-5d7d3cfc8d02">


The badge system serves multiple purposes:
- It provides a sense of achievement and progress
- Encourages users to diversify their contributions across different task types
- Potentially increases user retention by setting incremental goals

If I were to design a new badge, I might create one called "Consistency Champion." Users could earn Level 1 of this badge by completing at least one task every day for a week. This would encourage regular engagement with the platform.

## Data Quality and Verification

When faced with ambiguous cases, I opted to make educated guesses rather than skipping the tasks. This approach, while potentially introducing some noise into the data, aligns with the likely strategy employed by Google to filter and verify contributions.

Google likely verifies the reliability of annotators through consensus mechanisms, comparing responses across multiple users for the same image. To design a system that filters out unreliable annotators, one could:

1. Implement honeypot tasks with known answers to gauge accuracy
2. Track user response times to identify suspiciously fast submissions
3. Analyze patterns in user responses to detect automated or random inputs
4. Gradually increase task complexity for users who consistently provide accurate answers

## Platform Economics and User Acquisition

Unlike Amazon Mechanical Turk, Google Crowdsource doesn't offer monetary incentives. This approach likely stems from several factors:

1. Task Simplicity: The tasks are generally quick and don't require specialized skills
2. Volume Strategy: Google aims for a high volume of contributions, which would be costly to incentivize financially
3. Data Value: The individual value of each contribution is relatively low, but becomes significant in aggregate
4. User Engagement: The platform leverages intrinsic motivation and gamification instead of financial rewards

However, this model presents challenges in user acquisition and retention. As someone who hadn't heard of Google Crowdsource before this assignment, I believe Google could improve platform visibility by:

1. Integrating Crowdsource tasks with reCAPTCHA, exposing more users to the platform
2. Promoting the platform within other Google services
3. Offering Google Play credits or similar non-monetary rewards for consistent contributors

## Contribution to Google Services

The tasks on Google Crowdsource directly contribute to improving various Google products:

- Image Recognition: Enhances Google Images search accuracy
- Image Captioning: Improves accessibility features and image-based search
- Handwriting Recognition: Refines Google Lens and document digitization capabilities
- Facial Expression Recognition: Potentially aids in developing more sophisticated image processing algorithms
- Landmark Identification: Enhances Google Maps and travel-related services

## Overall User Experience

As a user, I appreciated the conciseness and simplicity of the tasks on Google Crowdsource. The platform's strength lies in its straightforward approach to data collection and user engagement through gamification.

However, the limited task variety during my participation period was disappointing. A more diverse range of tasks could make the experience more engaging and educational for users. Additionally, the platform's low visibility is a significant drawback. Google could benefit from more actively promoting Crowdsource to its vast user base.

In conclusion, while Google Crowdsource offers an efficient and user-friendly platform for contributing to AI development, there's potential for improvement in task diversity, user acquisition strategies, and integration with Google's broader ecosystem of services.

# Mercor

## The Sign-Up and AI Interview Process

The Mercor sign-up process took approximately 40 minutes, including a 20-minute AI-led interview. This process required submitting a resume and participating in the video interview. The streamlined approach offers efficiency in the hiring process. However, I haven't received a match yet from Mercor based on my interview.

### AI Interview Experience

The AI interviewer demonstrated impressive capabilities:
- Captured key points from my responses
- Formulated relevant follow-up questions that dived deep into technical difficulties
- Showed attentiveness to detail (and abiltiy to formulate questions based on detail)

However, the interview had limitations:
- It overly focused on one aspect of my experience
- It lacked diversity in question types (key issue)
- Did not allow for a comprehensive showcase of skills

### Comparison to Human-Led Interviews

While the AI interview felt less pressured and allowed for free expression, it lacked the nuanced interaction of a human interviewer:
- More rigid in structure
- Abrupt ending when time ran out
- Limited ability to adapt to candidate's full range of experiences (or analyzing from the resume)

## Ethical Implications and AI in Hiring

The use of AI in the hiring process, in my opinion, raises several ethical considerations:

1. **Fairness and Bias**: Can AI truly evaluate candidates fairly without human oversight?
2. **Data Privacy**: How is interview data stored and used?
3. **Job Displacement**: Will AI interviewers replace human HR professionals?

Mercor's approach, while innovative, needs careful examination to ensure it doesn't perpetuate biases or overlook crucial human elements in hiring.

## AI and Crowdsourcing: Potential Synergies

The integration of AI in Mercor's hiring process offers insights into how AI could enhance crowdsourcing platforms:

1. **Task Matching**: AI could efficiently match workers with suitable tasks based on their skills and experience.
2. **Quality Control**: AI could monitor task completion and provide real-time feedback to workers.
3. **Skill Development**: AI could identify skill gaps and recommend training opportunities.

However, the limitations observed in the Mercor interview suggest that AI should complement, not replace, human judgment in crowdsourcing management.

## Recommendations for Improvement

Based on the Mercor experience, here are suggestions for enhancing AI-driven platforms in crowdsourcing:

1. **Hybrid Approach**: Combine AI screening with human review for a more comprehensive evaluation.
2. **Customizable Interviews**: Allow candidates to select focus areas or roles before the AI interview.
3. **Diverse Question Sets**: Implement a wider range of question types to assess various skills and experiences.
4. **Transparent AI Use**: Clearly communicate how AI is used in the process and its limitations.
5. **Feedback Loop**: Provide candidates with insights from their AI interview to aid in their job search and skill development.

## Broader Implications for the Labor Market

The Mercor model showcases both the potential and limitations of AI in hiring and, by extension, in managing crowdsourced labor:

1. **Efficiency vs. Depth**: AI can quickly process large numbers of candidates but may miss subtleties that human recruiters catch.
2. **Skill Evaluation**: Certain skills, especially soft skills, may be challenging for AI to assess accurately.
3. **Evolving Job Roles**: As AI takes over routine screening tasks, human HR roles may shift towards more strategic, interpersonal aspects of hiring and worker management.

## Conclusion:

While Mercor's AI-driven approach offers a glimpse into the future of hiring and potentially crowdsourcing management, it also highlights the need for a balanced approach. As platforms evolve, they should strive to harness AI's efficiency while preserving the human elements crucial to fair and comprehensive evaluation. The ideal future for crowdsourcing platforms may lie in a hybrid model where AI handles routine tasks and initial screenings, while human managers focus on nuanced decision-making, worker development, and ensuring ethical practices. This approach could lead to more efficient, fair, and satisfying experiences for both workers and task providers in the crowdsourcing ecosystem.
