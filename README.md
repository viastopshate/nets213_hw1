Nets 2130 Homework 1 Writeup

# Zooniverse

## Project Selection and Tasks

I selected six diverse projects on Zooniverse to explore different aspects of crowdsourcing relating to sciences. Here is a screenshot of the progress of my projects:

<img width="1257" alt="Screenshot 2024-09-29 at 10 07 52 AM" src="https://github.com/user-attachments/assets/4c97f28c-065e-4d3e-a8a7-396e8b7b798f">


1. **Exosteroids and Galaxy Zoo**: From my initial research, astronomy-focused projects are among Zooniverse's most popular and well-known (and ones that lead to the most profound results). These are the projects that can be most representative to begin with. The two specific projects involve volunteers identifying stars and asteroids, which very much suits my interest. 

2. **Shark Spy and Wildlife Watch**: As a wildlife enthusiast, I chose these projects to apply my animal knowledge (such as birdwatching) to classification tasks. They offered insight into how crowdsourcing works in biodiversity research.

3. **Sudan Road Access**: From class, I was very impressed with how crowdsourcing can help with projects in underdeveloped countries. This project stood out for its real-world impact, helping to map roads and infrastructure for aid delivery in Sudan. I wish to understand how crowdsourcing can directly benefit humanitarian efforts through this project.

4. **Reading Emotions**: This psychology-related project caught my attention as it aligns with my minor in psychology. It involves more complex, and human centric tasks compared to simple classification. I would also like to know whether humans can be effective in sentimental analysis even after the emergence of generative AI> 

The tasks primarily involved classification, which humans can often perform more accurately than machines in certain contexts. For example:

- Identifying sharks or birds in images (Shark Spy and Wildlife Watch)
- Classifying galaxy types and asteroid characteristics (Galaxy Zoo and Exosteroids)
- Inferring emotions from written statements (Reading Emotions)
- Identifying infrastructure features like bridges and unpaved roads in satellite imagery (Sudan Road Access)

These tasks highlight a key strength of human crowdsourcing: the ability to make nuanced judgments that may be challenging for AI. For instance, distinguishing between a shark and other fish in murky water, or interpreting the emotional subtext of a written statement.

## Task Instructions and User Experience

Most project instructions were clear and easy to follow, with step-by-step guides available for each task. The UI is also very friendly: whenever a volunteer opens a task, he or she would be directed to the instructions which in clude forward and backward buttons to show each step consecutively. Moreover, the instructions also include multiple images for better comprehension. However, I encountered some usability issues:

1. In Shark Spy and Wildlife Watch, while the basic instructions were clear, identifying specific species required clicking through multiple images for reference. It's very difficult for me to identify the sharks as there is only a single image provided for each species and the images are very small. This process could be streamlined.

<img width="1306" alt="Screenshot 2024-09-29 at 9 40 07 AM" src="https://github.com/user-attachments/assets/f085a607-c90b-4986-b75f-307f84cd86e0">


2. The Wildlife Watch interface presented an unnecessarily large set of animal options, most of which were irrelevant to the actual tasks. This cluttered the UI and made bird identification more cumbersome than necessary. Moreover, I have to click in the species name to find the image of a species, which is very inconvenient. 

<img width="1305" alt="Screenshot 2024-09-29 at 9 21 17 AM" src="https://github.com/user-attachments/assets/88f74d10-978b-40c0-bfb8-83fa9a69260d">

<img width="423" alt="Screenshot 2024-09-29 at 9 30 48 AM" src="https://github.com/user-attachments/assets/d885eebc-d109-4981-bf43-fbb85c30d126">


3. The Sudan Road Access project initially posed challenges in distinguishing between roads and rivers in satellite imagery. This highlights the importance of providing clear visual examples and guidelines for more complex tasks.

<img width="1100" alt="Screenshot 2024-09-29 at 11 18 49 AM" src="https://github.com/user-attachments/assets/f14fbce0-9145-4661-8881-3059b8886dfd">


Despite these minor issues, Zooniverse generally provides a user-friendly experience with easily accessible instructions and intuitive interfaces for most projects.

## Completed Projects and Publications

Zooniverse's model of citizen science has led to numerous scientific publications. One notable example is the discovery of "Green Pea" galaxies, detailed in the paper "Galaxy Zoo Green Peas: discovery of a class of compact extremely star-forming galaxies."

<img width="638" alt="Screenshot 2024-09-29 at 11 20 02 AM" src="https://github.com/user-attachments/assets/0a09c87c-e668-43af-9afe-1f814fca7444">


This project showcases the unique advantage of human classifiers in scientific research. Volunteers were able to identify and investigate unusual objects that might have been overlooked by automated systems. The collective effort of citizen scientists helped establish the common characteristics of these rare galaxies, leading to a significant astronomical discovery.

This example illustrates how crowdsourcing can:

1. Leverage human pattern recognition skills for scientific breakthroughs
2. Enable large-scale data analysis that would be impractical for a small research team
3. Engage the public in the scientific process, potentially increasing science literacy and interest

## Time Investment

I spent approximately 30-40 minutes on most projects. For less engaging ones, I spend about 1t minutes. Individual tasks typically took around 15 seconds to complete.

This time investment reveals both strengths and limitations of the Zooniverse model:

- **Strength**: Quick, bite-sized tasks allow for easy participation and rapid data collection.
- **Limitation**: The repetitive nature of tasks can lead to fatigue or boredom, potentially affecting long-term volunteer retention.

## Volunteer Motivation

Through my experience, I realize that Zooniverse relies entirely on volunteers. This is very different from Amazon Mechanical Turk and can result in Zooniverse having a completely different audience than Turk. From my opinion, some of the points of motivation could be:

1. **Scientific Impact**: Many projects directly contribute to research, giving volunteers a sense of purpose and connection to scientific discovery. People choosing Zooniverse alreadya are those who don't care for making an income out of crowdsourcing (which differs a lot from Turk) and these impactful projects can serve as a strong motivation for them to continue. 

2. **Real-World Applications**: Projects like Sudan Road Access offer tangible humanitarian benefits. These benefits can often times not be measurable with money and is more suitable for a non-profit platform like Zooniverse.

3. **User Experience**: Zooniverse's very well-designed interface and clear instructions lower the barrier to entry and make participation enjoyable. This contrasts sharply to Turk's old-fashion design. Moreover, the tasks are very easily navigable and one can easily look for tasks of similar topics. 

4. **Topic Interest**: The diverse range of projects allows volunteers to contribute to fields they're passionate about, from astronomy to wildlife conservation.

5. **Eady Signups**: Signing up is very easy and straightforward.


However, the platform faces challenges in maintaining long-term engagement. The repetitive nature of tasks can lead to boredom, as I experienced after about 30 minutes of continuous work. To address this, Zooniverse could consider:

- Implementing more varied task types within projects
- Offering more frequent feedback on the impact of contributions
- Developing a robust community system to foster social connections among volunteers

## Ethical Considerations and Future Directions

While Zooniverse's volunteer model avoids many of the ethical concerns associated with paid crowdsourcing platforms, it raises other considerations:

1. **Data Privacy**: How is volunteer-submitted data protected and used?
2. **Intellectual Property**: How are volunteers credited for their contributions to scientific discoveries?
3. **Inclusivity**: Are there barriers to participation that might exclude certain groups?

Looking ahead, Zooniverse and similar platforms could explore:

1. Enhanced feedback mechanisms to show volunteers the direct impact of their work
2. Integration of machine learning to complement human efforts and tackle more complex tasks
3. Expanded educational resources to deepen volunteers' understanding of the scientific process
4. Partnerships with schools and universities to integrate citizen science into curricula

In conclusion, Zooniverse demonstrates the potential of citizen science crowdsourcing to advance scientific research and address real-world challenges. By leveraging human cognitive abilities and intrinsic motivation, it offers a unique model that complements other forms of crowdsourcing and traditional research methodologies.
